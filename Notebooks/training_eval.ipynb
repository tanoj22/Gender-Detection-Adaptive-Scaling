{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6db6bcb-a019-4871-ba46-d34f0db8c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, random, time, copy\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "# tqdm: avoid notebook widget; fall back to plain\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **k): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6756212b-2158-4068-bcc0-e580d5946d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Config\n",
    "# ------------------\n",
    "DATA_DIR = r\"D:\\utk_gender_balanced_6000\"\n",
    "SAVE_DIR = \"./checkpoints_fas_utk\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0  # Windows-safe\n",
    "MAX_EPOCHS = 30\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "BASE_LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTH = 0.0  # standard CE smoothing; handled outside FAS\n",
    "\n",
    "# FAS hyperparams\n",
    "FAS_C = 0.5            # blend between individual li and group beta\n",
    "FAS_EMA_ALPHA = 0.9    # per-sample EMA of loss\n",
    "FAS_CLIP_MIN = 0.2     # clamp weights\n",
    "FAS_CLIP_MAX = 5.0\n",
    "FAS_BETA_LR_SCALE = 0.1  # slower LR for beta\n",
    "\n",
    "# EarlyStopping composite score: balanced_acc - GAP_W * |acc_f - acc_m|\n",
    "GAP_W = 0.25\n",
    "\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.0  # set >0 if you want a separate test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8895c1-e151-49a7-aa63-c201dcdf27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------\n",
    "# Utils\n",
    "# ------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "\n",
    "def parse_gender_from_name(name: str) -> Optional[int]:\n",
    "    # UTKFace filename: age_gender_race_date.jpg  (gender: 0 male, 1 female)\n",
    "    # robust parse\n",
    "    base = os.path.basename(name)\n",
    "    m = re.match(r\"(\\d+)_(\\d)_(\\d)_\", base)\n",
    "    if m:\n",
    "        return int(m.group(2))\n",
    "    return None\n",
    "\n",
    "def discover_images(data_dir: str) -> List[Tuple[str,int]]:\n",
    "    \"\"\"Return list of (path, gender_label). Supports:\n",
    "       1) subfolders like .../male, .../female or .../0, .../1\n",
    "       2) UTKFace filename convention\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    data_dir = Path(data_dir)\n",
    "\n",
    "    # Case 1: subfolders\n",
    "    subs = [p for p in data_dir.iterdir() if p.is_dir()]\n",
    "    sub_ok = False\n",
    "    if subs:\n",
    "        mapping = {}\n",
    "        for sub in subs:\n",
    "            key = sub.name.lower()\n",
    "            if key in (\"male\", \"m\", \"0\"): mapping[sub] = 0; sub_ok = True\n",
    "            if key in (\"female\", \"f\", \"1\"): mapping[sub] = 1; sub_ok = True\n",
    "        if sub_ok:\n",
    "            for sub, lab in mapping.items():\n",
    "                for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.webp\"):\n",
    "                    for f in sub.rglob(ext):\n",
    "                        paths.append((str(f), lab))\n",
    "    # Case 2: filename parse\n",
    "    if not sub_ok:\n",
    "        for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.webp\"):\n",
    "            for f in data_dir.rglob(ext):\n",
    "                g = parse_gender_from_name(f.name)\n",
    "                if g is not None:\n",
    "                    paths.append((str(f), g))\n",
    "\n",
    "    if not paths:\n",
    "        raise RuntimeError(f\"No images found or labels not parsed in: {data_dir}\")\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95680845-32be-467b-8ba1-b1c13c4a6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Dataset\n",
    "# ------------------\n",
    "class UTKGenderDataset(Dataset):\n",
    "    def __init__(self, items: List[Tuple[str,int]], transform=None):\n",
    "        self.items = items\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, y = self.items[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform: img = self.transform(img)\n",
    "        # group = gender here (0 male, 1 female)\n",
    "        return img, torch.tensor(y, dtype=torch.long), torch.tensor(y, dtype=torch.long), torch.tensor(idx, dtype=torch.long)\n",
    "\n",
    "# ------------------\n",
    "# Transforms\n",
    "# ------------------\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d48e9721-d1e0-4fe5-ac18-4fbec593c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Split data\n",
    "# ------------------\n",
    "all_items = discover_images(DATA_DIR)\n",
    "random.shuffle(all_items)\n",
    "n = len(all_items)\n",
    "n_test = int(n * TEST_SPLIT)\n",
    "n_val = int(n * VAL_SPLIT)\n",
    "\n",
    "test_items = all_items[:n_test]\n",
    "val_items = all_items[n_test:n_test+n_val]\n",
    "train_items = all_items[n_test+n_val:]\n",
    "\n",
    "train_ds = UTKGenderDataset(train_items, train_tfms)\n",
    "val_ds = UTKGenderDataset(val_items, val_tfms)\n",
    "test_ds = UTKGenderDataset(test_items, val_tfms) if n_test>0 else None\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True) if test_ds else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2326bae0-147e-4c95-b3fb-ac09ce790d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Model: ResNet18 + Dropout\n",
    "# ------------------\n",
    "class ResNet18WithDropout(nn.Module):\n",
    "    def __init__(self, pretrained=True, p_drop=0.3, n_classes=2):\n",
    "        super().__init__()\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        in_feat = m.fc.in_features\n",
    "        # replace head with dropout + linear\n",
    "        m.fc = nn.Sequential(\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(in_feat, n_classes)\n",
    "        )\n",
    "        self.net = m\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "model = ResNet18WithDropout(pretrained=True, p_drop=0.3, n_classes=2).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67fff05f-09ce-46dd-85f2-4984ef5e0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Fair Adaptive Scaling (FAS) Loss\n",
    "# ------------------\n",
    "class FairAdaptiveScalingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines individual scaling l_i (EMA of past loss per-sample) and\n",
    "    learnable group scaling beta[g], then weights standard CE loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_groups:int, num_samples:int, c:float=0.5, ema_alpha:float=0.9,\n",
    "                 clip_min:float=0.2, clip_max:float=5.0):\n",
    "        super().__init__()\n",
    "        assert 0 <= c <= 1\n",
    "        self.num_groups = num_groups\n",
    "        self.c = c\n",
    "        self.ema_alpha = ema_alpha\n",
    "        self.clip_min = clip_min\n",
    "        self.clip_max = clip_max\n",
    "\n",
    "        # per-sample EMA of loss\n",
    "        self.register_buffer(\"ema_loss\", torch.zeros(num_samples, dtype=torch.float32))\n",
    "\n",
    "        # learnable group weights beta\n",
    "        self.beta = nn.Parameter(torch.ones(num_groups, dtype=torch.float32))\n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_ema(self, idxs: torch.Tensor, losses: torch.Tensor):\n",
    "        # ema[idx] = alpha*ema + (1-alpha)*current\n",
    "        self.ema_loss[idxs] = self.ema_alpha * self.ema_loss[idxs] + (1 - self.ema_alpha) * losses.detach()\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor, groups: torch.Tensor, idxs: torch.Tensor):\n",
    "        # base per-sample loss\n",
    "        per_sample = self.ce(logits, targets)  # [B]\n",
    "\n",
    "        # update EMA (before using)\n",
    "        with torch.no_grad():\n",
    "            self.update_ema(idxs, per_sample)\n",
    "\n",
    "        # individual scaling l_i from EMA of loss (normalize by batch mean to be stable)\n",
    "        with torch.no_grad():\n",
    "            ema_vals = self.ema_loss[idxs]  # [B]\n",
    "            batch_mean = ema_vals.mean().clamp(min=1e-8)\n",
    "            l_i = (ema_vals / batch_mean)\n",
    "\n",
    "        # group scaling beta_g (learnable)\n",
    "        beta_g = self.beta[groups]  # [B]\n",
    "\n",
    "        # combine\n",
    "        c_i = self.c * l_i + (1 - self.c) * beta_g\n",
    "        c_i = c_i.clamp(self.clip_min, self.clip_max)\n",
    "\n",
    "        # weighted loss\n",
    "        loss = (c_i * per_sample).mean()\n",
    "        return loss, c_i.detach(), per_sample.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "264138a3-2b9a-4fe6-b5bd-20599ab0b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Metrics\n",
    "# ------------------\n",
    "@torch.no_grad()\n",
    "def accuracies(y_true: torch.Tensor, y_pred: torch.Tensor, g: torch.Tensor) -> Dict[str, float]:\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    g = g.cpu().numpy()\n",
    "    overall = (y_true == y_pred).mean()\n",
    "    acc0 = (y_pred[g==0] == y_true[g==0]).mean() if np.any(g==0) else np.nan\n",
    "    acc1 = (y_pred[g==1] == y_true[g==1]).mean() if np.any(g==1) else np.nan\n",
    "    # balanced class accuracy (over labels 0/1)\n",
    "    b0 = (y_pred[y_true==0] == 0).mean() if np.any(y_true==0) else np.nan\n",
    "    b1 = (y_pred[y_true==1] == 1).mean() if np.any(y_true==1) else np.nan\n",
    "    bacc = np.nanmean([b0,b1])\n",
    "    # worst-group accuracy over gender\n",
    "    wg = np.nanmin([acc0, acc1])\n",
    "    gap = np.abs(acc0 - acc1) if (not np.isnan(acc0) and not np.isnan(acc1)) else np.nan\n",
    "    return dict(overall=overall, male=acc0, female=acc1, balanced=bacc, worst_group=wg, gap=gap)\n",
    "\n",
    "def composite_score(balanced_acc: float, gap: float, gap_w: float=GAP_W) -> float:\n",
    "    if np.isnan(gap): gap = 0.0\n",
    "    return float(balanced_acc - gap_w * gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68e43936-8939-43d8-b8f0-89fcbb1c4676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S.SAI\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ------------------\n",
    "# Train setup\n",
    "# ------------------\n",
    "train_len = len(train_ds)\n",
    "num_groups = 2  # gender\n",
    "fas_loss = FairAdaptiveScalingLoss(num_groups=num_groups, num_samples=train_len,\n",
    "                                   c=FAS_C, ema_alpha=FAS_EMA_ALPHA,\n",
    "                                   clip_min=FAS_CLIP_MIN, clip_max=FAS_CLIP_MAX).to(DEVICE)\n",
    "\n",
    "# CE for reporting (unweighted)\n",
    "ce_report = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Optimizer with smaller LR for beta\n",
    "params = [\n",
    "    {\"params\": [p for n,p in model.named_parameters() if p.requires_grad], \"lr\": BASE_LR, \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": fas_loss.beta, \"lr\": BASE_LR * FAS_BETA_LR_SCALE, \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "best_state = None\n",
    "best_score = -1e9\n",
    "epochs_no_improve = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65b2c45f-bafe-4ae5-bf00-cd3371b9c481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████| 75/75 [11:31<00:00,  9.22s/it, loss=0.7089]\n",
      "Val: 100%|████████████████████████████████████████████████████████████████| 19/19 [00:55<00:00,  2.94s/it, loss=0.4397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc=0.796 | val_acc=0.847 | val_bal=0.846 | val_gap=0.033 | val_worst=0.830 | score=0.8379\n",
      "\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████| 75/75 [11:35<00:00,  9.27s/it, loss=0.4616]\n",
      "Val: 100%|████████████████████████████████████████████████████████████████| 19/19 [01:01<00:00,  3.23s/it, loss=0.4379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc=0.865 | val_acc=0.861 | val_bal=0.863 | val_gap=0.143 | val_worst=0.792 | score=0.8273\n",
      "\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████| 75/75 [11:31<00:00,  9.21s/it, loss=0.3964]\n",
      "Val: 100%|████████████████████████████████████████████████████████████████| 19/19 [00:56<00:00,  2.98s/it, loss=0.4889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc=0.896 | val_acc=0.861 | val_bal=0.863 | val_gap=0.130 | val_worst=0.798 | score=0.8305\n",
      "\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████| 75/75 [11:30<00:00,  9.21s/it, loss=0.3901]\n",
      "Val: 100%|████████████████████████████████████████████████████████████████| 19/19 [00:58<00:00,  3.06s/it, loss=0.4363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc=0.900 | val_acc=0.874 | val_bal=0.874 | val_gap=0.004 | val_worst=0.872 | score=0.8733\n",
      "\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████| 75/75 [11:17<00:00,  9.03s/it, loss=0.3497]\n",
      "Val: 100%|████████████████████████████████████████████████████████████████| 19/19 [00:56<00:00,  2.97s/it, loss=0.4831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc=0.911 | val_acc=0.859 | val_bal=0.858 | val_gap=0.074 | val_worst=0.821 | score=0.8395\n",
      "\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████| 75/75 [11:32<00:00,  9.24s/it, loss=0.3383]\n",
      "Val: 100%|████████████████████████████████████████████████████████████████| 19/19 [00:57<00:00,  3.01s/it, loss=0.7810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc=0.915 | val_acc=0.781 | val_bal=0.786 | val_gap=0.321 | val_worst=0.625 | score=0.7056\n",
      "\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████| 75/75 [11:37<00:00,  9.29s/it, loss=0.3234]\n",
      "Val: 100%|████████████████████████████████████████████████████████████████| 19/19 [00:56<00:00,  3.00s/it, loss=0.4635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc=0.915 | val_acc=0.868 | val_bal=0.865 | val_gap=0.144 | val_worst=0.793 | score=0.8293\n",
      "\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████| 75/75 [11:38<00:00,  9.31s/it, loss=0.2319]\n",
      "Val: 100%|████████████████████████████████████████████████████████████████| 19/19 [00:57<00:00,  3.02s/it, loss=0.4963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc=0.950 | val_acc=0.886 | val_bal=0.884 | val_gap=0.096 | val_worst=0.836 | score=0.8604\n",
      "\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████████████████████████████████████████████████████████| 75/75 [11:44<00:00,  9.39s/it, loss=0.1730]\n",
      "Val: 100%|████████████████████████████████████████████████████████████████| 19/19 [00:58<00:00,  3.07s/it, loss=0.6549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc=0.961 | val_acc=0.882 | val_bal=0.884 | val_gap=0.101 | val_worst=0.834 | score=0.8589\n",
      "Early stopping at epoch 9. Best score: 0.8733\n",
      "Loaded best checkpoint from epoch 4 (score=0.8733)\n",
      "\n",
      "==== Final Validation (Best) ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val: 100%|████████████████████████████████████████████████████████████████| 19/19 [00:55<00:00,  2.93s/it, loss=0.4721]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall: 0.8742\n",
      "male: 0.8724\n",
      "female: 0.8761\n",
      "balanced: 0.8742\n",
      "worst_group: 0.8724\n",
      "gap: 0.0037\n",
      "loss: 0.4721\n",
      "ce: 0.2896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------\n",
    "# Training / Validation loops\n",
    "# ------------------\n",
    "def run_epoch(loader, model, criterion_fas, train=True):\n",
    "    if train: model.train()\n",
    "    else: model.eval()\n",
    "    y_true, y_pred, g_all = [], [], []\n",
    "    total_loss, total_ce = 0.0, 0.0\n",
    "    n_total = 0\n",
    "\n",
    "    it = tqdm(loader, desc=(\"Train\" if train else \"Val\"))\n",
    "    for imgs, ys, gs, idxs in it:\n",
    "        imgs, ys, gs, idxs = imgs.to(DEVICE), ys.to(DEVICE), gs.to(DEVICE), idxs.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(imgs)\n",
    "            if LABEL_SMOOTH > 0 and train:\n",
    "                # Optional CE smoothing just for reporting (FAS already uses CE internally)\n",
    "                # We still keep criterion inside FAS as standard CE.\n",
    "                targets = F.one_hot(ys, num_classes=2).float()\n",
    "                ls_logits = logits.log_softmax(dim=1)\n",
    "                smooth = (1 - LABEL_SMOOTH) * targets + LABEL_SMOOTH / 2\n",
    "                ce_loss = -(smooth * ls_logits).sum(dim=1).mean()\n",
    "            else:\n",
    "                ce_loss = ce_report(logits, ys)\n",
    "\n",
    "            if train:\n",
    "                loss, weights, per_sample = criterion_fas(logits, ys, gs, idxs)\n",
    "            else:\n",
    "                # during eval, don't update EMA -> do a forward without update\n",
    "                # simple hack: call criterion but stop EMA update via no_grad outer (already not train)\n",
    "                loss, weights, per_sample = criterion_fas(logits, ys, gs, idxs)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item()) * imgs.size(0)\n",
    "        total_ce   += float(ce_loss.item()) * imgs.size(0)\n",
    "        n_total    += imgs.size(0)\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        y_true.append(ys.detach())\n",
    "        y_pred.append(preds.detach())\n",
    "        g_all.append(gs.detach())\n",
    "\n",
    "        it.set_postfix(loss=f\"{total_loss/n_total:.4f}\")\n",
    "\n",
    "    y_true = torch.cat(y_true)\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    g_all  = torch.cat(g_all)\n",
    "\n",
    "    mets = accuracies(y_true, y_pred, g_all)\n",
    "    mets.update({\n",
    "        \"loss\": total_loss / max(n_total,1),\n",
    "        \"ce\": total_ce / max(n_total,1),\n",
    "    })\n",
    "    return mets\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    print(f\"\\nEpoch {epoch}/{MAX_EPOCHS}\")\n",
    "\n",
    "    train_m = run_epoch(train_loader, model, fas_loss, train=True)\n",
    "    val_m   = run_epoch(val_loader,   model, fas_loss, train=False)\n",
    "\n",
    "    # Composite early-stopping score\n",
    "    val_score = composite_score(val_m[\"balanced\"], val_m[\"gap\"], GAP_W)\n",
    "    scheduler.step(val_score)\n",
    "\n",
    "    row = {\"epoch\": epoch, \"lr\": optimizer.param_groups[0][\"lr\"], \n",
    "           **{f\"train_{k}\":v for k,v in train_m.items()},\n",
    "           **{f\"val_{k}\":v for k,v in val_m.items()},\n",
    "           \"val_score\": val_score}\n",
    "    history.append(row)\n",
    "\n",
    "    # Log concise line\n",
    "    print(f\"train_acc={train_m['overall']:.3f} | val_acc={val_m['overall']:.3f} | \"\n",
    "          f\"val_bal={val_m['balanced']:.3f} | val_gap={val_m['gap']:.3f} | \"\n",
    "          f\"val_worst={val_m['worst_group']:.3f} | score={val_score:.4f}\")\n",
    "\n",
    "    # Early stopping on best composite score\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        best_state = {\"model\": copy.deepcopy(model.state_dict()),\n",
    "                      \"fas\": copy.deepcopy(fas_loss.state_dict()),\n",
    "                      \"epoch\": epoch,\n",
    "                      \"score\": best_score}\n",
    "        torch.save(best_state, os.path.join(SAVE_DIR, \"best.pt\"))\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best score: {best_score:.4f}\")\n",
    "            break\n",
    "\n",
    "# Load best weights\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state[\"model\"])\n",
    "    fas_loss.load_state_dict(best_state[\"fas\"])\n",
    "    print(f\"Loaded best checkpoint from epoch {best_state['epoch']} (score={best_state['score']:.4f})\")\n",
    "\n",
    "# Final validation summary\n",
    "print(\"\\n==== Final Validation (Best) ====\")\n",
    "val_best = run_epoch(val_loader, model, fas_loss, train=False)\n",
    "for k,v in val_best.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Optional: Test set\n",
    "if test_loader is not None:\n",
    "    print(\"\\n==== Test ====\")\n",
    "    test_m = run_epoch(test_loader, model, fas_loss, train=False)\n",
    "    for k,v in test_m.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3bfc8a9-8dd3-44ff-903c-bccecfe78feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val: 100%|████████████████████████████████████████████████████████████████| 16/16 [00:42<00:00,  2.67s/it, loss=0.3310]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== External TEST ====\n",
      "overall:   0.8799\n",
      "male (0):  0.8819\n",
      "female(1): 0.8775\n",
      "balanced:  0.8797\n",
      "worst_grp: 0.8775\n",
      "gap:       0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== Robust TEST evaluation (works with/without 'per_group' in run_epoch output) ====\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "assert test_loader is not None, \"test_loader is missing. Build it before running this cell.\"\n",
    "\n",
    "\n",
    "test_m = run_epoch(test_loader, model, fas_loss, train=False)\n",
    "\n",
    "\n",
    "overall   = test_m.get(\"overall\")\n",
    "male_acc  = test_m.get(\"male\")\n",
    "female_acc= test_m.get(\"female\")\n",
    "balanced  = test_m.get(\"balanced\")\n",
    "worst     = test_m.get(\"worst_group\")\n",
    "gap       = test_m.get(\"gap\")\n",
    "\n",
    "# If any of male/female/balanced/worst/gap is missing, recompute from raw preds\n",
    "need_recompute = any(v is None for v in [male_acc, female_acc, balanced, worst, gap])\n",
    "\n",
    "if need_recompute:\n",
    "    ys_all, yhat_all, g_all = [], [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, ys, gs, idxs in test_loader:\n",
    "            preds = model(imgs.to(DEVICE)).argmax(1).cpu().numpy()\n",
    "            ys_all.extend(ys.numpy().tolist())\n",
    "            yhat_all.extend(preds.tolist())\n",
    "            g_all.extend(gs.numpy().tolist())\n",
    "\n",
    "    ys_all   = np.array(ys_all)\n",
    "    yhat_all = np.array(yhat_all)\n",
    "    g_all    = np.array(g_all)\n",
    "\n",
    "    if overall is None:\n",
    "        overall = float((ys_all == yhat_all).mean())\n",
    "\n",
    "    # class-wise (0 = male, 1 = female)\n",
    "    m_mask = ys_all == 0\n",
    "    f_mask = ys_all == 1\n",
    "    male_acc   = float((yhat_all[m_mask] == ys_all[m_mask]).mean()) if m_mask.any() else float(\"nan\")\n",
    "    female_acc = float((yhat_all[f_mask] == ys_all[f_mask]).mean()) if f_mask.any() else float(\"nan\")\n",
    "\n",
    "    if balanced is None:\n",
    "        balanced = float(np.nanmean([male_acc, female_acc]))\n",
    "    if worst is None:\n",
    "        worst = float(np.nanmin([male_acc, female_acc]))\n",
    "    if gap is None:\n",
    "        gap = float(abs(male_acc - female_acc))\n",
    "\n",
    "\n",
    "print(\"\\n==== External TEST ====\")\n",
    "print(f\"overall:   {overall:.4f}\")\n",
    "print(f\"male (0):  {male_acc:.4f}\")\n",
    "print(f\"female(1): {female_acc:.4f}\")\n",
    "print(f\"balanced:  {balanced:.4f}\")\n",
    "print(f\"worst_grp: {worst:.4f}\")\n",
    "print(f\"gap:       {gap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b605163-9bdf-4b79-ac1a-7a571f0ad4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
